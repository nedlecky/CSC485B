{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedlecky/CSC485B/blob/main/CSC485_130_PythagorasInSteps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSC 485B Spring 2023: CSC485_130_PythagorasInSteps using MLP\n",
        "## Solving the Pythagoras problem in small pieces to make it work\n",
        "### Input the length of the two sides, ML computes hypotenuse, perimeter, and area\n",
        "* SUNY Plattsburgh, Spring 2023\n",
        "* Dr. Ned Lecky\n",
        "* nleck001@plattsburgh.edu\n",
        "* ned@lecky.com\n",
        "\n",
        "# Work In Progress\n",
        "# This is the version we walked through and discussed in class 2/23/2023\n",
        "# Most of the tricky problems are solved here but we're not in final form\n",
        "\n",
        "Assignment:\n",
        "* Follow the architectural idea from Class 04: Slides 17-18 as discussed in class\n",
        "* Build your “Pythagoras dataset” to include 400 random triangles with sizes from 2cm to 2000cm on each side\n",
        "* Split it into training and test portions using sklearn.model_selection.train_test_split\n",
        "* Scale using std_scaler and figure out how to use a sklearn.pipeline so you don’t have to keep remembering to scale all of your Xs\n",
        "* Get your super simple MLPs running as suggested on slide 17- Success is: max error < 0.5cm on hypotenuse, < 1cm on perimeter, and < 1cm^2 on area\n",
        "* Now, make the data more “realistic” by adding small random errors to your dataset\n",
        "  * Generate an exact dataset\n",
        "  * Now make your training dataset by assuming there are small errors in side1 and side2 measurements\n",
        "* Does your same code work when run on the exact data?\n",
        "\n"
      ],
      "metadata": {
        "id": "QBwjvn9X7lgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Support Functions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# This makes us reproducible (and we can adjust fixed_seed to get different results)\n",
        "fixed_seed = 1\n",
        "np.random.seed(fixed_seed)\n",
        "\n",
        "# Return n random floats between lo and hi as 1-column NumPy matrix\n",
        "def rand_nlohi(n=1, lo=0, hi=1):\n",
        "  # This is just a uniform distribution from lo to hi... we can adjust if appropriate in the future\n",
        "  return (np.random.rand(n) * (hi - lo) + lo).reshape(-1,1)\n",
        "\n",
        "# Often a good idea as long as we are keeping values near +/- 1... don't need exponential notation\n",
        "np.set_printoptions(floatmode='fixed', precision=4, suppress=True)\n",
        "# This will get us all 400 rows printed... which fails past 40 x 2 columns\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# Simple numpy array print with optional push to file\n",
        "def nprint(m, name='', also_write_file=False):\n",
        "  print(f\"{name} {m.shape} {m.dtype}\")\n",
        "  print(m)\n",
        "  if also_write_file and name != '':\n",
        "    fprint(m, name)\n",
        "\n",
        "# Print numpy array to file (needs name)\n",
        "def fprint(m, name=''):\n",
        "  if name != '':\n",
        "    with open(name, 'w') as f:\n",
        "      print(f\"{name} {m.shape} {m.dtype}\", file=f)\n",
        "      print(m, file=f)\n",
        "  else:\n",
        "    print('fprint needs a name!')\n",
        "\n",
        "# Remove a file and don't complain if it doesn't exist\n",
        "def remove_file(name):\n",
        "  try:\n",
        "    os.remove(name)\n",
        "  except:\n",
        "    return\n",
        "\n",
        "remove_file('X')\n",
        "remove_file('Y')\n",
        "remove_file('Y_pred')\n",
        "remove_file('XY')\n",
        "remove_file('X_train')\n",
        "remove_file('X_train_scaled')\n",
        "remove_file('X_test')\n",
        "remove_file('Y_train')\n",
        "remove_file('Y_test')\n",
        "remove_file('Y_testY_pred')\n",
        "\n",
        "remove_file('X1')\n",
        "remove_file('X1_train')\n",
        "remove_file('X1_train_scaled')\n",
        "remove_file('X1_test')\n",
        "remove_file('Y1')\n",
        "remove_file('Y1_pred')\n",
        "remove_file('Y1_train')\n",
        "remove_file('Y1_test')\n",
        "remove_file('X1Y1')\n",
        "remove_file('Y1_testY1_pred')\n",
        "\n",
        "\n",
        "remove_file('X2')\n",
        "remove_file('X2_train')\n",
        "remove_file('X2_train_scaled')\n",
        "remove_file('X2_test')\n",
        "remove_file('Y2')\n",
        "remove_file('Y2_pred')\n",
        "remove_file('Y2_train')\n",
        "remove_file('Y2_test')\n",
        "remove_file('X2Y2')\n",
        "remove_file('Y2_testY2_pred')\n",
        "\n"
      ],
      "metadata": {
        "id": "0B_bKL2AjY7z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Test Data X\n",
        "## 400 Triangles with side1 side2 spread from 2 to 2000 cm\n"
      ],
      "metadata": {
        "id": "YL7kMtd55FpS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nedp4wU8pITb"
      },
      "outputs": [],
      "source": [
        "# This is the full test input data for right triangles\n",
        "# Reminder: Final input is the length of the two sides, output is length of hypotenuse, perimeter, and area\n",
        "# x = [side1, side2]\n",
        "# y = [hypotenuse, perimeter, area]\n",
        "import math\n",
        "\n",
        "# Setup what you want to generate\n",
        "N = 400\n",
        "use_trivial_test_data = False  # Force use of just 2 easy triangle\n",
        "make_lengths_random = True     # Random or just stepped in size\n",
        "split_data = True              # Split test/train 50/50? (else test=train)\n",
        "\n",
        "# Big help if we avoid BIG numbers... so lets use cm/100 as our unit\n",
        "# NOTE: This is because we are squaring and then scaling...\n",
        "# If we scale and then square, things might work fine (in a pipeline?)\n",
        "shortest_side = 0.02 # 2 cm\n",
        "longest_side = 20.00 # 2000 cm\n",
        "\n",
        "# Generate X\n",
        "if(use_trivial_test_data):\n",
        "  # Trivial Test Data\n",
        "  X = np.array([\n",
        "    [math.sqrt(2)/2, math.sqrt(2)/2],\n",
        "    [1, 1]\n",
        "  ])\n",
        "else:\n",
        "  if(make_lengths_random):\n",
        "    # This makes random X\n",
        "    side1 = rand_nlohi(N, shortest_side, longest_side)\n",
        "    side2 = rand_nlohi(N, shortest_side, longest_side)\n",
        "  else:\n",
        "    # OR: This makes more regular X, all equilateral triangles\n",
        "    side1 = np.arange(shortest_side, longest_side, longest_side/N).reshape(-1,1)\n",
        "    side2 = np.arange(shortest_side, longest_side, longest_side/N).reshape(-1,1)\n",
        "  X = np.hstack([side1, side2])\n",
        "\n",
        "fprint(X,'X')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make expected Y"
      ],
      "metadata": {
        "id": "-hwvKXRZRQ1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's compute the FULL X,Y expected results\n",
        "# Reminder: We tell you the length of the two sides, you compute length of hypotenuse, perimeter, and area\n",
        "# x = [side1, side2]\n",
        "# y = [hypotenuse, perimeter, area]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "hypotenuse = np.sqrt(np.square(X[:,0:1]) + np.square(X[:,1:2]))\n",
        "perimeter = X[:,0:1] + X[:,1:2] + hypotenuse\n",
        "area = (X[:,0:1] * X[:,1:2]) / 2.\n",
        "Y = np.hstack([hypotenuse, perimeter, area])\n",
        "fprint(Y,'Y')\n",
        "fprint(np.hstack([X,Y]), 'XY')\n",
        "\n",
        "# Optional noise in Y\n",
        "# Not tested yet!\n",
        "# Just bump all up or down by up to 1%\n",
        "#Y = Y * (100. + (np.random.rand(Y.shape[0],Y.shape[1])-0.5))/100.\n",
        "\n",
        "# And here are the full train/test sets if we want to try the old way\n",
        "if(split_data):\n",
        "  (X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.5, random_state=1)\n",
        "else:\n",
        "  # OR we can have train=test!\n",
        "  X_train = X_test = X\n",
        "  Y_train = Y_test = Y\n",
        "\n",
        "fprint(X_train, 'X_train')\n",
        "fprint(X_test, 'X_test')\n",
        "fprint(Y_train, 'Y_train')\n",
        "fprint(Y_test, 'Y_test')\n"
      ],
      "metadata": {
        "id": "w0drnB6qkR9v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make X1 and Y1 for First MLP"
      ],
      "metadata": {
        "id": "FLOMBNQj-XCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want X1 to be [side1 side2 side1^2 side2^2]\n",
        "# And Y1 to be [hypotenuse^2, area]\n",
        "\n",
        "# Note... to get rid of manual scaling, I need to scale side1 and side2 and then compute\n",
        "#  the squares of the SCALED values...\n",
        "X1 = np.hstack([X, np.square(X)])\n",
        "Y1 = np.hstack([np.square(Y[:, 0:1]), Y[:,2:3]])\n",
        "\n",
        "fprint(X1, 'X1')\n",
        "fprint(Y1, 'Y1')\n",
        "fprint(np.hstack([X1,Y1]), 'X1Y1')\n",
        "\n",
        "if(split_data):\n",
        "  (X1_train, X1_test, Y1_train, Y1_test) = train_test_split(X1, Y1, test_size=0.5, random_state=1)\n",
        "else:\n",
        "  # OR we can have train=test!\n",
        "  X1_train = X1_test = X1\n",
        "  Y1_train = Y1_test = Y1\n",
        "\n",
        "fprint(X1_train, 'X1_train')\n",
        "fprint(X1_test, 'X1_test')\n",
        "fprint(Y1_train, 'Y1_train')\n",
        "fprint(Y1_test, 'Y1_test')\n"
      ],
      "metadata": {
        "id": "zFoXMwl8-ewm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the MLP scale/train\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "def scale(X):\n",
        "  scaler = preprocessing.StandardScaler().fit(X)\n",
        "  X_scaled = scaler.transform(X)\n",
        "  return scaler, X_scaled\n",
        "\n",
        "def train(X, Y, hidden_layer_sizes=(10,10),\n",
        "            activation='relu', max_iter=10000, title='Predictions'):\n",
        "  \n",
        "  regr = MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
        "                     hidden_layer_sizes=hidden_layer_sizes,\n",
        "                     activation=activation,\n",
        "                     max_iter=max_iter,\n",
        "                     random_state=1,\n",
        "                      verbose=True)\n",
        "  regr.fit(X, Y)\n",
        "  return regr\n",
        "\n",
        "def test(regr, scaler, X, Y):\n",
        "  # The reshape below forces Y_pred to come back as 1 column if there is ony 1 output\n",
        "  Y_pred = regr.predict(scaler.transform(X)).reshape(Y.shape[0],-1)\n",
        "\n",
        "  # Metrics...\n",
        "  print(f\"Mean squared error: {mean_squared_error(Y, Y_pred):.2f}\")\n",
        "  print(f\"Mean absolute error: {mean_absolute_error(Y, Y_pred):.2f}\")\n",
        "  print(f\"Mean absolute percentage error: {mean_absolute_percentage_error(Y, Y_pred):.2f}\")\n",
        "\n",
        "  # The Pandas describe\n",
        "  df = pd.DataFrame(data = Y_pred - Y)\n",
        "  print(df.describe())\n",
        "\n",
        "  return Y_pred\n",
        "\n",
        "(scaler1, X1_train_scaled) = scale(X1_train)\n",
        "regr1 = train(X1_train_scaled, Y1_train)\n",
        "Y1_pred = test(regr1, scaler1, X1_test, Y1_test)\n",
        "\n",
        "fprint(X1_train_scaled,'X1_train_scaled')\n",
        "fprint(Y1_pred,'Y1_pred')\n",
        "fprint(np.hstack([Y1_test, Y1_pred]),'Y1_testY1_pred')\n"
      ],
      "metadata": {
        "id": "a6lnQycVIwYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82406ab2-786b-48ec-b756-58b2af79acf8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 0.54\n",
            "Mean absolute error: 0.35\n",
            "Mean absolute percentage error: 0.05\n",
            "                0           1\n",
            "count  200.000000  200.000000\n",
            "mean    -0.000043    0.172421\n",
            "std      0.025138    1.029203\n",
            "min     -0.080464   -1.853392\n",
            "25%     -0.012425   -0.484213\n",
            "50%      0.001073   -0.066694\n",
            "75%      0.008340    0.531677\n",
            "max      0.160578    6.158041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That looks pretty good... let's try some examples not from the dataset\n",
        "X_111 = np.array([1,1,1,1]).reshape(1,-1)\n",
        "nprint(X_111,'X_111')\n",
        "\n",
        "# Don't forget to scale the inputs the same way we did for training!\n",
        "Y_pred = regr1.predict(scaler1.transform(X_111))\n",
        "nprint(Y_pred,'Y_pred 111')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipm2x89mM1bw",
        "outputId": "3531b0ef-f73b-452a-97bf-c9384bde67db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_111 (1, 4) int64\n",
            "[[1 1 1 1]]\n",
            "Y_pred 111 (1, 2) float64\n",
            "[[2.0506 0.8893]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make X2 and Y2 for Second MLP"
      ],
      "metadata": {
        "id": "l_XxWIIHKcSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want X2 to be [side1 side2 side3] = [X1 , sqrt(Y1)]\n",
        "# And Y2 to be [perim]\n",
        "X2 = np.hstack([X, np.sqrt(Y1[:,0:1]).reshape(-1,1)])\n",
        "Y2 = Y[:,1:2]\n",
        "\n",
        "fprint(X2, 'X2')\n",
        "fprint(Y2, 'Y2')\n",
        "fprint(np.hstack([X2,Y2]), 'X2Y2')\n",
        "\n",
        "if(split_data):\n",
        "  (X2_train, X2_test, Y2_train, Y2_test) = train_test_split(X2, Y2, test_size=0.5, random_state=1)\n",
        "else:\n",
        "  # OR we can have train=test!\n",
        "  X2_train = X2_test = X2\n",
        "  Y2_train = Y2_test = Y2\n",
        "\n",
        "fprint(X2_train, 'X2_train')\n",
        "fprint(X2_test, 'X2_test')\n",
        "fprint(Y2_train, 'Y2_train')\n",
        "fprint(Y2_test, 'Y2_test')\n"
      ],
      "metadata": {
        "id": "px55XPkJKlIP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Test\n",
        "(scaler2, X2_train_scaled) = scale(X2_train)\n",
        "regr2 = train(X2_train_scaled, Y2_train)\n",
        "Y2_pred = test(regr2, scaler2, X2_test, Y2_test).reshape(-1,1)\n",
        "\n",
        "fprint(X2_train_scaled,'X2_train_scaled')\n",
        "fprint(Y2_pred,'Y2_pred')\n",
        "fprint(np.hstack([Y2_test, Y2_pred]),'Y2_testY2_pred')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVOJSzm0JiJw",
        "outputId": "c618cba8-e688-4ca2-c773-43f6eb607299"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 0.00\n",
            "Mean absolute error: 0.00\n",
            "Mean absolute percentage error: 0.00\n",
            "                0\n",
            "count  200.000000\n",
            "mean    -0.000041\n",
            "std      0.003738\n",
            "min     -0.028780\n",
            "25%     -0.000097\n",
            "50%     -0.000023\n",
            "75%      0.000058\n",
            "max      0.009726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That looks pretty good... let's try some examples not from the dataset\n",
        "X_771 = np.array([0.707,0.707,1]).reshape(1,-1)\n",
        "nprint(X_771,'X_771')\n",
        "\n",
        "# Don't forget to scale the inputs the same way we did for training!\n",
        "Y_pred = regr2.predict(scaler2.transform(X_771))\n",
        "nprint(Y_pred,'Y_pred_X_771')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbd57c5-0d96-4120-8a93-490548008706",
        "id": "4uHWCx8-h9vk"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_771 (1, 3) float64\n",
            "[[0.7070 0.7070 1.0000]]\n",
            "Y_pred_X_771 (1,) float64\n",
            "[2.4032]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How about another try at all-in-one?"
      ],
      "metadata": {
        "id": "AP9m5U--Q8_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# And if we tried doing it all in one go?\n",
        "(scaler_all, X_train_scaled) = scale(X_train)\n",
        "regr_all = train(X_train_scaled, Y_train)\n",
        "Y_pred = test(regr_all, scaler_all, X_test, Y_test)\n",
        "\n",
        "fprint(X_train_scaled,'X_train_scaled')\n",
        "fprint(Y_pred,'Y_pred')\n",
        "fprint(np.hstack([Y_test, Y_pred]),'Y_testY_pred')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPSZuFggiUdc",
        "outputId": "34ab0012-cbc6-443e-e0e8-9e730eced4bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 1.19\n",
            "Mean absolute error: 0.57\n",
            "Mean absolute percentage error: 0.08\n",
            "                0           1           2\n",
            "count  200.000000  200.000000  200.000000\n",
            "mean    -0.019011   -0.020642    0.063669\n",
            "std      0.288596    0.286767    1.845103\n",
            "min     -1.159927   -1.456972   -5.290061\n",
            "25%     -0.176990   -0.159293   -0.880957\n",
            "50%      0.001990   -0.003237   -0.168737\n",
            "75%      0.147682    0.155962    0.866702\n",
            "max      0.725209    0.740662    9.328812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That looks pretty good... let's try some examples not from the dataset\n",
        "X_771 = np.array([0.707,0.707]).reshape(1,-1)\n",
        "nprint(X_771,'X_771')\n",
        "\n",
        "# Don't forget to scale the inputs the same way we did for training!\n",
        "Y_pred = regr_all.predict(scaler_all.transform(X_771))\n",
        "nprint(Y_pred,'Y_pred_X_771')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e056252-9808-4515-fe5c-cd57423074f5",
        "id": "Hs4guj7nrB_t"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_771 (1, 2) float64\n",
            "[[0.7070 0.7070]]\n",
            "Y_pred_X_771 (1, 3) float64\n",
            "[[ 1.3471  2.5991 -0.5311]]\n"
          ]
        }
      ]
    }
  ]
}